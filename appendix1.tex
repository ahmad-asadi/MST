\chapter*{‌پیوست}
\markboth{پیوست}{}
\addcontentsline{toc}{chapter}{پیوست}
در این بخش قصد داریم سورس کد اصلی برنامه را به صورت دقیق مورد بررسی قرار دهیم. در ادامه، ابتدا نگاه کوتاهی بر ساختار فایل ایجاد شده در این پروژه انداخته و سپس ماژول‌های کلیدی برنامه را مورد بررسی قرار می‌دهیم.

\section*{ساختار فایل}
در این پروژه، سه بسته\enfootnote{Package} مختلف به شرح زیر ایجاد شده است.
\begin{enumerate}
	\item بسته \lr{data\_utils} \\
	این بسته شامل یک ماژول به نام \lr{data\_helper} است که امکاناتی را برای سهولت در کار با مجموعه‌‌داده مورد استفاده فراهم می‌نماید. این ماژول تنها یک کلاس به نام \lr{COCOHelper} را در خود جای داده که تمامی توابع مورد نیاز برای کار با مجموعه‌داده \lr{MSCOCO} را فراهم می‌نماید.
	\item بسته \lr{main.neuralnetworks.cnn} \\
	این بسته شامل یک ماژول به نام \lr{CNN} و یک بسته داخلی دیگر به نام \lr{inception} است. وظیفه این بسته، فراهم‌کردن امکانات مربوط به رمزگذار پروژه می‌باشد. از آن‌جا که ممکن است در آینده به بیش از یک مدل به عنوان رمزگذار نیاز شود، در این بسته از الگوی طراحی\enfootnote{Design Pattern} نما\enfootnote{Facade} استفاده شده است. کلاس \lr{CNN} نقش نما را در این بسته ایفا می‌نماید. بسته داخلی \lr{inception}، مدل از پیش آموزش دیده شده \lr{Google Inception V3} را دانلود کرده و مورد استفاده قرار می‌دهد.
	\item بسته \lr{main.neuralnetworks.rnn} \\
	این بسته شامل ماژول‌های زیر می‌باشد:
	\begin{enumerate}
		\item ماژول \lr{StackedRNN} \\
		این ماژول شامل یک کلاس با همین نام به منظور پیاده‌سازی شبکه رمزگشا در پروژه است. 
		\item ماژول \lr{RNNUtils} \\
		این ماژول شامل دو کلاس است. کلاس اول با نام \lr{RNNUtils} وظیفه فراهم‌سازی برخی از متد‌های مرتبط با شبکه رمزگذار را بر عهده دارد. کلاس دیگری که در این ماژول ایجاد شده است، کلاسی با نام \lr{RNNOptions} است که شامل تمامی ورودی‌های و تنظیمات زمان اجرای مورد نیاز برای کار با شبکه رمزگشا می‌باشد.
	\end{enumerate}
\end{enumerate}
کد اصلی پروژه در یک فایل پایتون با نام \lr{image\_caption\_train.py} در ریشه سیستم فایل، قرار دارد. در بخش‌های بعدی به بررسی کد مربوط به هر یک از این بخش‌ها می‌پردازیم.

\section*{بررسی کد‌های پروژه}
\subsection*{فایل \lr{image\_caption\_train.py} }

در ابتدای این فایل، کتابخانه‌ها و ماژول‌های مورد استفاده، معرفی شده‌اند. برای این کار از قطعه کد زیر استفاده شده است.
\begin{latin}
\begin{minted}{python}
import tensorflow as tf
import numpy as np
import nltk
import os
import time
import resource
import math
import copy
import re
import logging

from data_utils import data_helper
from main.neuralNetworks.cnn import CNN as Encoder
from main.neuralNetworks.rnn import StackedRNN as Decoder
from main.neuralNetworks.rnn import RNNUtils as DecoderUtils

from matplotlib import pyplot as plt
from multiprocessing import Process
\end{minted}
\end{latin}

ساختار این پروژه به شکلی است که داده‌های موجود در مجموعه‌داده به صورت دسته‌ای\enfootnote{batch} در حافظه بارگذاری می‌شوند. بارگذاری داده‌ها در حافظه، توسط \lr{CPU} انجام می‌شود. وقتی یک دسته از داده‌ها در حافظه بارگذاری شد، پردازنده گرافیکی می‌تواند این دسته از داده‌ها را پردازش نموده و شبکه را آموزش دهد. در زمانی که پردازنده گرافیکی در حال پردازش داده‌های بارگذاری شده و آموزش شبکه است، \lr{CPU} غیرفعال است. 
\\
برای بهبود کارایی سیستم، فرایند بارگذاری داده‌ها و آموزش شبکه،‌در این پروژه به صورت موازی انجام شده است. به همین منظور، توابعی برای بارگذاری یک دسته از داده‌ها و آموزش شبکه توسط یک دسته داده بارگذاری شده، به طور جداگانه نوشته شده‌اند. این توابع، در فرایندهای\enfootnote{Process} جداگانه و به طور موازی با هم اجرا می‌شوند.
\\
در ادامه کدهای مربوط به بارگذاری یک دسته از داده‌ها را نمایش داده‌ایم.

\begin{latin}
\begin{minted}[tabsize=2,obeytabs,showtabs]{python}
def getNextBatch(batchId, rawCaptions, cocoHelper, rnnOptions, vocab, word2ind):
  batchInstanceCount = min(rnnOptions.batch_insts, len(rawCaptions))
  batchData = np.zeros(shape=(rnnOptions.time_step, 
  cocoHelper.word2vec.layer1_size, rnnOptions.batch_size))
  batchLabel = np.zeros(shape=(rnnOptions.time_step, 
  rnnOptions.word_embedding_size, rnnOptions.batch_size))
  batchImgs = []
  for i in range(batchInstanceCount - 1):
    embeddedCaptionInsts, instImgFileNames, embeddedLabels = 
    getNextInstance(i + batchId, rawCaptions, cocoHelper,
                    rnnOptions.rnn_utils, vocab, word2ind=word2ind)
    cnt = 0
    offset = i * 5
    for embeddedCaptionInst in embeddedCaptionInsts:
      batchData[1:embeddedCaptionInst.shape[0], :, offset + cnt] = 
      embeddedCaptionInst[0:min(rnnOptions.time_step, 
      embeddedCaptionInst.shape[0]) - 1]
      cnt += 1
    cnt = 0
    for embeddedLabel in embeddedLabels:
      batchLabel[0:embeddedLabel.shape[0], :, offset + cnt] = 
      embeddedLabel[0:rnnOptions.time_step]
      cnt += 1
    for instImgFileName in instImgFileNames:
      batchImgs.append(instImgFileName)
    return batchData, batchImgs, batchLabel


def getNextInstance(iteration, data, cocoHelper, rnnUtils, vocab, word2ind):
  global testLabel
  inst_image = [cocoHelper.imgs[i] for i in cocoHelper.imgs][iteration % len(data)]
  dataInsts = [cocoHelper.anns[i] for i in cocoHelper.anns if cocoHelper.anns[i]
  ["image_id"] == inst_image["id"]]
  embeddedCaptions = [rnnUtils.embed_inst_to_vocab(dataInst=
  dataInsts[i], cocoHelper=cocoHelper) for i in range(len(dataInsts))]
  embeddedLabels = [rnnUtils.embed_inst_to_vocab(dataInst=dataInsts[i], 
  cocoHelper=cocoHelper) for i in range(len(dataInsts))]
  inst_image_filenames = [inst_image["file_name"] for _ in range(len(dataInsts))]
  testLabel.append(dataInsts)	
  return embeddedCaptions, inst_image_filenames, embeddedLabels
  
def extractNextBatch(batchId, capWord2Ind, captionsDict, cocoHelper, rawCaptions,
                      rnnOptions, validation_mode=False):
  global batch_data_buffer, batch_img_filename_buffer, batch_label_buffer
  global test_batch_data_buffer, test_batch_img_filename_buffer 
  global test_batch_label_buffer
  batch_data, batch_img_filename, batch_label = getNextBatch(batchId=batchId,
  rawCaptions=rawCaptions,
  cocoHelper=cocoHelper,
  rnnOptions=rnnOptions,
  vocab=captionsDict,
  word2ind=capWord2Ind)
  if not validation_mode:
    batch_data_buffer = batch_data
    batch_img_filename_buffer = batch_img_filename
    batch_label_buffer = batch_label
  else:
    test_batch_data_buffer = batch_data
    test_batch_img_filename_buffer = batch_img_filename
    test_batch_label_buffer = batch_label
 
\end{minted}
\end{latin}

همین‌طور، تابع مربوط به آماده‌سازی داد‌ه‌‌ها و آموزش شبکه به صورت زیر نوشته شده‌ است.
\begin{latin}
\begin{minted}{python}
def prepare_data_and_train_structure(batchData, i, batchImgFileName, cnn,
            data_dir, imageFeaturesSize, rnn, rnnOptions, batchLabelRaw):
  global costs
  batchInput = np.zeros(shape=(batchData.shape[0], batchData.shape[2], 
  batchData.shape[1]))
  batchLabel = np.zeros(shape=(batchLabelRaw.shape[0], batchLabelRaw.shape[2]
  , batchLabelRaw.shape[1]))
  batchImageFeatures = np.zeros(shape=(batchData.shape[2],
  rnnOptions.image_feature_size[0], rnnOptions.image_feature_size[1]))
  for batchCnt in range(len(batchImgFileName)):
  imageFeatures = cnn.extract_features(os.path.join(
  data_dir, "train2014/" + batchImgFileName[batchCnt]))
  imageFeatures = imageFeatures.reshape(-1, rnnOptions.image_feature_size[-1])
  batchInput[:, batchCnt, :] = batchData[:, :, batchCnt]
  batchImageFeatures[batchCnt, :, :] = imageFeatures
  batchLabel[0:batchLabel.shape[0] - 1, batchCnt, :] = 
  [batchLabelRaw[i + 1, :, batchCnt] for i in
  range(batchLabelRaw.shape[0] - 1)]

  costs[i] = rnn.train_batch(Xbatch=batchInput,
   annotationsBatch=batchImageFeatures, Ybatch=batchLabel, keep_prob=0.5)


\end{minted}	
\end{latin}

تابع اصلی پروژه که فراخوانی و انجام فعالیت‌های اصلی بر عهده آن است، به شکل زیر نوشته شده است. این تابع، اولین تابعی است که شروع به اجرا شدن می‌نماید. خطوط ابتدایی این تابع، عملیات تنظیم محدودیت‌های سیستمی را بر عهده دارند.

\begin{latin}
\begin{minted}{python}
def start():
  global testLabel
  testLabel = []
  rsrc = resource.RLIMIT_DATA
  soft, hard = resource.getrlimit(rsrc)
  print('Soft limit starts as  :', soft)
  resource.setrlimit(rsrc, (32 * 1024 * 1024 * 1024, hard))  # limit to one kilobyte
  # set logger to print logs in console
  logging.getLogger().addHandler(logging.StreamHandler())  
  soft, hard = resource.getrlimit(rsrc)
  print('Soft limit changed to :', soft)
  plt.interactive(True)
  print("defining constants")
  data_dir, imageFeaturesSize, rnnUtils, maxIterCount = 
  initializeParameters()
  print("loading dataset")
  cocoHelper, captionsDict, rawCaptions, capWord2Ind, capInd2Word = 
  loadDataset(data_dir, rnnUtils)
  test_cocoHelper, test_captionsDict, test_rawCaptions, test_capWord2Ind,
  test_capInd2Word = loadTestDataset(data_dir, rnnUtils)
  print("creating encoder's structure")
  cnn = createEncoder(data_dir)
  print("creating decoder's structure and initialization")
  rnn, rnnOptions = createAndInitializeDecoder(captionsDict, imageFeaturesSize,
  rnnUtils, word_embedding_size=cocoHelper.word2vec.layer1_size, cnn=cnn)
  printGraph(rnnOptions, cnn=cnn.net.graph)
  extractNextBatch(0, capWord2Ind, captionsDict, cocoHelper, rawCaptions, rnnOptions)
  batchData = copy.copy(batch_data_buffer)
  batchImgFileName = list(batch_img_filename_buffer)
  batchLabelRaw = copy.copy(batch_label_buffer)
  batchInput = np.zeros(shape=(batchData.shape[0], batchData.shape[2],
  batchData.shape[1]))
  batchLabel = np.zeros(shape=(batchLabelRaw.shape[0], batchLabelRaw.shape[2], 
  batchLabelRaw.shape[1]))
  batchImageFeatures = np.zeros(shape=(batchData.shape[2], 
  rnnOptions.image_feature_size[0], rnnOptions.image_feature_size[1]))
  for batchCnt in range(len(batchImgFileName)):
    imageFeatures = cnn.extract_features(os.path.join(data_dir, "train2014/" 
    + batchImgFileName[batchCnt]))
    imageFeatures = imageFeatures.reshape(-1, rnnOptions.image_feature_size[-1])
    batchInput[:, batchCnt, :] = batchData[:, :, batchCnt]
    batchImageFeatures[batchCnt, :, :] = imageFeatures
    batchLabel[0:batchLabel.shape[0] - 1, batchCnt, :] = [
    batchLabelRaw[i + 1, :, batchCnt] for i in
    range(batchLabelRaw.shape[0] - 1)
    ]
    print("test image name: ", batchImgFileName[0])
    print("starting to train the structure")
    global costs
    startTime = time.time()
    costs = np.zeros(maxIterCount)
    checkPoint = 1
    statShowPeriod = 1

  batchId = 0
  for i in range(maxIterCount):
    loop_counter = math.ceil((len(rawCaptions) / 5) / rnnOptions.batch_size)
    loop_counter = min(loop_counter, 10)
    for internal_loop_counter in range(loop_counter):
      print("iteration:" + repr(i) + ", internal loop: processing ",
      100 * internal_loop_counter / loop_counter,
      "%")
      batchData = copy.copy(batch_data_buffer)
      batchImgFileName = list(batch_img_filename_buffer)
      batchLabelRaw = copy.copy(batch_label_buffer)
      load_data_process = Process(target=extractNextBatch, args=(batchId, 
      capWord2Ind, captionsDict, cocoHelper, rawCaptions, rnnOptions))
      load_data_process.start()
    prepare_data_and_train_structure(batchData=batchData, i=i, 
    batchImgFileName=batchImgFileName, cnn=cnn,
    data_dir=data_dir, imageFeaturesSize=imageFeaturesSize, rnn=rnn,
    rnnOptions=rnnOptions, batchLabelRaw=batchLabelRaw)
    load_data_process.join()
    batchId += 1
    endTime = time.time()
    duration = (endTime - startTime) / statShowPeriod
    print("batch ", i, ", train time per batch: ", duration, " 
    current gained cost: ", costs[i])
    startTime = endTime
    print("drawing error curve...")
    if i % statShowPeriod == 0 and i > 0:
      plt.plot([i - 1, i], costs[i - 1:i + 1])
      plt.show()
      plt.pause(1)
      if i % checkPoint == 0:
        print("saving current model...")
        rnnOptions.saver.save(rnnOptions.session, 
        rnnOptions.saved_model_path)
        print("testing current model:")
        testModel(rnn, rnnOptions, test_cocoHelper, 
        test_capInd2Word, test_capWord2Ind, test_captionsDict,
        test_rawCaptions, cnn, data_dir)

\end{minted}
\end{latin}

تابع \lr{test\_model}، تابعی است که در آن، مدل آموزش داده شده، برای تولید شرح متناظر یک دسته تصویر، مورد استفاده قرار می‌گیرد. این تابع به شکل زیر نوشته شده است.

\begin{latin}
\begin{minted}{python}
def testModel(rnn, rnnOptions, cocoHelper, ind2word, word2ind, vocab,
               raw_captions, cnn, data_dir):
  global testLabel
  testLabel = []
  extractNextBatch(0, capWord2Ind=word2ind, captionsDict=vocab, 
  cocoHelper=cocoHelper, rawCaptions=raw_captions,
  rnnOptions=rnnOptions, validation_mode=True)
  batchData = copy.copy(test_batch_data_buffer)
  batchImgFileName = list(test_batch_img_filename_buffer)
  batchLabelRaw = copy.copy(test_batch_label_buffer)
  batchInput = np.zeros(shape=(batchData.shape[0], batchData.shape[2], 
  batchData.shape[1]))
  batchLabel = np.zeros(shape=(batchLabelRaw.shape[0], batchLabelRaw.shape[2],
  batchLabelRaw.shape[1]))
  batchImageFeatures = np.zeros(shape=(batchData.shape[2], 
  rnnOptions.image_feature_size[0], rnnOptions.image_feature_size[1]))
  for batchCnt in range(len(batchImgFileName)):
    imageFeatures = cnn.extract_features(os.path.join(data_dir, "val2014/" 
    + batchImgFileName[batchCnt]))
    imageFeatures = imageFeatures.reshape(-1, rnnOptions.image_feature_size[-1])
    batchInput[:, batchCnt, :] = batchData[:, :, batchCnt]
    batchImageFeatures[batchCnt, :, :] = imageFeatures
    batchLabel[0:batchLabel.shape[0] - 1, batchCnt, :] = [
    batchLabelRaw[i + 1, :, batchCnt] for i in
    range(batchLabelRaw.shape[0] - 1)
    ]
  predicted_captions = ["" for _ in range(rnnOptions.batch_insts)]
  BLEUScores = np.zeros(shape=rnnOptions.batch_insts)
  test_input = np.zeros(shape=(batchInput.shape[0], 1, batchInput.shape[2]))
  test_image_features = np.zeros(shape=(1, batchImageFeatures[1], 
  batchImageFeatures[2]))
  for batch_inst in range(rnnOptions.batch_insts - 1):
    i = batch_inst * 5
    test_input[:, 0, :] = batchInput[:, i, :]
    out = rnn.run_step(X=test_input, annotations=batchImageFeatures[i, :, :], 
    init_zero_state=True)[0][0]
    for testInd in range(rnnOptions.time_step - 1):
    new_word = cocoHelper.word2vec.most_similar(positive=[out], topn=1)[0][0]
    predicted_captions[batch_inst] += " " + new_word
    batchInput[testInd + 1, i, 0:cocoHelper.word2vec.layer1_size] = 
    cocoHelper.word2vec[new_word]
    test_input[:, 0, :] = batchInput[:, i, :]
    out = rnn.run_step(X=test_input, init_zero_state=False)[testInd + 1]
    out = out[0]
  BLEUScores[batch_inst] = nltk.translate.bleu_score.sentence_bleu(
  [ref["caption"] for ref in testLabel[batch_inst]],
  predicted_captions[batch_inst], emulate_multibleu=True)
  print("prediction: ", i, ":", predicted_captions[batch_inst])
  print("human label: ", testLabel[batch_inst])
  print("BLEU SCORE: ", BLEUScores[batch_inst])
\end{minted}
\end{latin}

در ادامه، توابع دیگری که در این فایل مورد استفاده قرار گرفته‌اند، آورده شده‌اند.

\begin{latin}
	\begin{minted}{python}

def printGraph(rnnOptions, cnn):
  print("writing graphs to /tmp/graph in order to show it in tensor board")
  writer = tf.summary.FileWriter("/tmp/graph")
  writer.add_graph(rnnOptions.session.graph)
  writer.add_graph(cnn)


def createAndInitializeDecoder(captionsDict, imageFeaturesSize, rnnUtils, 
                        word_embedding_size, cnn):
  rnnOptions = DecoderUtils.RNNOptions(vocab=captionsDict, rnnUtils=rnnUtils, 
  image_feature_size=imageFeaturesSize,
  word_embedding_size=word_embedding_size)
  rnn = Decoder.StackedRNN(input_size=rnnOptions.input_size, image_feature_size=
  rnnOptions.image_feature_size,
  lstm_size=rnnOptions.lstm_size, number_of_layers=rnnOptions.num_layers,
  output_size=rnnOptions.out_size, session=rnnOptions.session,
  learning_rate=rnnOptions.learning_rate, batch_size=rnnOptions.batch_size,
  name=rnnOptions.name, cnn_graph=cnn.net.session.graph)
  rnnOptions.session.run(tf.global_variables_initializer())
  rnnOptions.saver = tf.train.Saver(tf.global_variables())
  return rnn, rnnOptions


def createEncoder(data_dir):
  cnn = Encoder.CNN(os.path.join(data_dir, "train2014"))
  return cnn


def loadDataset(data_dir, rnnUtils):
  cocoHelper = data_helper.COCOHelper(data_dir + 
                                      "annotations/captions_train2014.json")
  rawCaptions, captionsDict, capIndToWord, capWordToInd, capIndToWord = 
  cocoHelper.extract_captions()
  sentenceSize=100,
  return cocoHelper, captionsDict, rawCaptions, capWordToInd, capIndToWord 


def loadTestDataset(data_dir, rnnUtils):
  cocoHelper = data_helper.COCOHelper(data_dir + "annotations/captions_val2014.json")
  rawCaptions, captionsDict, capIndToWord, capWordToInd, capIndToWord = 
  cocoHelper.extract_captions()
  return cocoHelper, captionsDict, rawCaptions, capWordToInd, capIndToWord  


def initializeParameters():
  rnnUtils = DecoderUtils.RNNUtils()
  data_dir = "../../Dataset/MS-COCO/"
  imageFeaturesSize = [64, 2048]
  maxIterCount = 20000
  return data_dir, imageFeaturesSize, rnnUtils, maxIterCount
\end{minted}
\end{latin}

در انتهای این فایل، با فراخوانی تابع \lr{start()}، برنامه آغاز به کار می‌نماید.

\subsection*{فایل \lr{StackedRNN.py}}


\begin{latin}
\begin{minted}{python}
import tensorflow as tf
import numpy as np

from tensorflow.contrib.seq2seq import BahdanauAttention
from main.neuralNetworks.rnn.GuidedAttentionRNNCell import GuidedAttentionCell


class StackedRNN:
  def __init__(self, input_size, image_feature_size, lstm_size, number_of_layers, 
              output_size, session, learning_rate, batch_size, attn_length=0, 
              attn_size=0, attn_mechanism="bahdanau", name="rnn", cnn_graph=None):
    self.scope = name
    self.cnn_graph = cnn_graph
    self.attn_length = attn_length
    self.attn_size = attn_size
    self.attn_mechanism = attn_mechanism
    self.input_size = input_size
    self.image_feature_size = image_feature_size
    self.batch_size = batch_size
    print("input size:", input_size)
    self.lstm_size = lstm_size
    self.number_of_layers = number_of_layers
    self.output_size = output_size
    self.session = session
    self.learning_rate = tf.constant(learning_rate)
    self.lstm_last_state = np.zeros(
                          shape=(self.number_of_layers * 2 * self.lstm_size,)
                          )

    with tf.device('/device:GPU:0'):
        with tf.variable_scope(name_or_scope=self.scope):
            with tf.variable_scope(name_or_scope="Input"):
                self.X = tf.placeholder(dtype=tf.float32, shape=(None, None,
                                                 self.input_size), name="X")
                self.annotations = tf.placeholder(dtype=tf.float32, shape=(None,
                                                 self.image_feature_size[0],
                                    self.image_feature_size[1]),
                                    name="annotations")
            with tf.variable_scope(name_or_scope="DropOutParams"):
                self.keep_prob = tf.placeholder(dtype=tf.float32)

            with tf.name_scope(name="attentionMechanism"):
              if self.attn_mechanism == "bahdanau":
                self.attention_mechanism = attention.BahdanauAttention(
                                                        num_units=self.attn_length,
                                                        memory=,
                                                        memory_sequence_length=)
              elif self.attn_mechanism == "luong":
                self.attention_mechanism = attention.LuongAttention(
                                                        num_units=self.attn_length,
                                                        memory=,
                                                        memory_sequence_length=)
              else:
                raise ValueError(
                "attention mechanism '%s' is not defined." % self.attn_mechanism)

            with tf.name_scope(name="RNN_Core"):
              self.lstm_cells = [GuidedAttentionCell(self.lstm_size, forget_bias=1.0,
                                                    state_is_tuple=True)
                                                    for _ in 
                                                    range(self.number_of_layers)]
              self.lstm_cells = [tf.nn.rnn_cell.DropoutWrapper(lstm_cell,
                                                    output_keep_prob=self.keep_prob)
                                                    for lstm_cell in self.lstm_cells]
              self.lstm = tf.nn.rnn_cell.MultiRNNCell(self.lstm_cells, 
                                                    state_is_tuple=True)
              self.lstm = tf.nn.rnn_cell.DropoutWrapper(self.lstm, 
                                            output_keep_prob=self.keep_prob)
              self.lstm_init_states = self.lstm.zero_state(
                                batch_size=tf.shape(self.X)[1], dtype=tf.float32)
              self.outputs, self.lstm_current_state = tf.nn.dynamic_rnn(
                                                    cell=self.lstm, inputs=self.X,
                                                    dtype=tf.float32, time_major=True,
                                                    initial_state=self.lstm_init_states)

            with tf.variable_scope(name_or_scope="Output"):
              self.OUT_W = tf.Variable(initial_value=
                        tf.random_normal(shape=(self.lstm_size, self.output_size),
                                        stddev=0.01, name="output_W"))
              self.OUT_B = tf.Variable(initial_value=
                                      tf.random_normal(shape=(self.output_size,),
                                        stddev=0.01, name="output_B"))

              self.outputs_reshaped = tf.reshape(tensor=self.outputs, 
                                            shape=[-1, self.lstm_size])
              self.net_out = tf.nn.batch_normalization(x=tf.matmul(
                                  self.outputs_reshaped, self.OUT_W) + self.OUT_B,
                                  mean=0, variance=1, offset=0, scale=1,
                                  variance_epsilon=1e-10)

              self.batch_time_shape = tf.shape(self.outputs)
              self.final_output = tf.reshape(self.net_out, 
                                              shape=(self.batch_time_shape[0],
                                              self.batch_time_shape[1],
                                              self.output_size))

              self.Y = tf.placeholder(dtype=tf.float32,
                                      shape=(None, None, self.output_size))

              self.Y_long = tf.reshape(tensor=self.Y, shape=(-1, self.output_size))

              self.cost = tf.reduce_sum(tf.losses.absolute_difference(
                                        predictions=self.softmax_net_out,
                                        labels=self.Y_long))

              self.train_op = tf.train.AdamOptimizer(
                              learning_rate=self.learning_rate).minimize(self.cost)
              self.print_number_of_parameters()

  def run_step(self, X, annotations=None, init_zero_state=True, keep_prob=1):
    if init_zero_state:
      init_value = np.zeros(shape=(self.number_of_layers * 2 * self.lstm_size,))
    else:
      init_value = self.lstm_last_state

    out, next_lstm_state = self.session.run([self.final_output, self.lstm_current_state],
                                            feed_dict={self.X: X,
                                                      self.annotations: annotations,
                                                      self.keep_prob: keep_prob})

    self.lstm_last_state = next_lstm_state[len(next_lstm_state) - 1]

    return out

  def train_batch(self, Xbatch, annotationsBatch, Ybatch, keep_prob=1):
    init_value = np.zeros(shape=(Xbatch.shape[0], 
            self.number_of_layers * 2 * self.lstm_size))

    cost, _ = self.session.run([self.cost, self.train_op], 
                                feed_dict={self.X: Xbatch, self.Y: Ybatch,
                                self.annotations: annotationsBatch,
                                self.keep_prob: keep_prob})

    return cost

  @staticmethod
  def print_number_of_parameters():
    total_parameters = 0
    for variable in tf.trainable_variables():
      shape = variable.get_shape()
      variable_parameters = 1
      for dim in shape:
        variable_parameters *= dim.value
      total_parameters += variable_parameters
    print("total number of parameters: ", total_parameters)

\end{minted}
\end{latin}




\subsection*{فایل \lr{GuidedAttentionRNNCell.py}}
ماژول \lr{GuidedAttentionRNNCell} دارای یک کلاس به همان نام است. این کلاس، واحد اصلی شبکه را پیاده‌سازی می‌نماید. هر واحد شبکه دیکودر، یک سلول \lr{LSTM} است، که مکانیزم توجه بصری ارائه شده را پیاده‌سازی می‌نماید. برای استفاده از امکانات موجود در کلاس \lr{BasicLSTMCell} در تنسورفلو، کلاس \lr{GuidedAttentionRNNCell} را به گونه‌ای ایجاد نموده‌ایم که از آن ارث‌بری نماید.
\begin{latin}
\begin{minted}{python}
import tensorflow as tf
from tensorflow.contrib.seq2seq.python.ops.attention_wrapper import AttentionWrapper

class GuidedAttentionCell(tf.nn.rnn_cell.BasicLSTMCell):
  def __init__(self, num_units, forget_bias=1.0,
               state_is_tuple=True, activation=None, reuse=None):
    super(GuidedAttentionCell, self).__init__(num_units=num_units, 
                                              forget_bias=forget_bias,
                                              state_is_tuple=state_is_tuple, 
                                              activation=activation,
                                              reuse=reuse)

  def __call__(self, inputs, state, scope=None):
    return super(GuidedAttentionCell, self).call(inputs=inputs, state=state)

\end{minted}
\end{latin}


\subsection*{آموزش مدل جاسازی کلمات}
برای آموزش جاسازی کلمات از کتابخانه \lr{Gensim} استفاده نموده‌ایم. این کتابخانه، جاسازی کلمات را مطابق با پژوهشی که در فصل سوم، به تفصیل مورد بررسی قرار گرفت، پیاده‌سازی نموده‌است. برای ایجاد جاسازی‌های مناسب برای کلمات، مدل مورد استفاده، تا جایی که خطای جاسازی به کمتر از 540000 برسد، روی مجموعه کلمات، آموزش داده می‌شود. برای کنترل برنامه در حالتی که خطای جاسازی کلمات، کمتر از مقدار مورد نظر نشود، سقف 1000000 تکرار، برای خاتمه آموزش، در نظر گرفته شده است. تابع \lr{create\_word2vec}، که کد آن در ادامه این بخش، آورده شده است، یکی از متدهای کلاس \lr{DataHelper} است که در زمان ایجاد نمونه جدید و در انتهای \lr{Constructor} این کلاس، فراخوانی می‌شود.
\begin{latin}
\begin{minted}{python}
def create_word2vec(self):
  print("creating word embeddings structure")
  sentences = [re.split("[\W]+", (self.anns[i]["caption"]).lower()) for i in self.anns]
  iteration_count = 1
  self.word2vec = Word2Vec(iter=iteration_count, min_count=0, size=512, workers=8)
  self.word2vec_vocab = self.word2vec.build_vocab(sentences=sentences)
  print("training word embeddings")
  loss_value = 1000000
  i = 0
  while loss_value >= 540000 and i < iteration_count:
    self.word2vec.train(sentences=sentences, total_examples=len(sentences), epochs=1,
    compute_loss=True)
    loss_value = self.word2vec.get_latest_training_loss()
    print("iteration:", i, ", loss value:", loss_value)
    i += 1
\end{minted}
\end{latin}